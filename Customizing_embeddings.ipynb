{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Vq31CdSRpgkI"
   },
   "source": [
    "# Customizing embeddings\n",
    "\n",
    "This notebook demonstrates one way to customize OpenAI embeddings to a particular task.\n",
    "\n",
    "The input is training data in the form of [text_1, text_2, label] where label is +1 if the pairs are similar and -1 if the pairs are dissimilar.\n",
    "\n",
    "The output is a matrix that you can use to multiply your embeddings. The product of this multiplication is a 'custom embedding' that will better emphasize aspects of the text relevant to your use case. In binary classification use cases, we've seen error rates drop by as much as 50%.\n",
    "\n",
    "In the following example, I use 1,000 sentence pairs picked from the SNLI corpus. Each pair of sentences are logically entailed (i.e., one implies the other). These pairs are our positives (label = 1). We generate synthetic negatives by combining sentences from different pairs, which are presumed to not be logically entailed (label = -1).\n",
    "\n",
    "For a clustering use case, you can generate positives by creating pairs from texts in the same clusters and generate negatives by creating pairs from sentences in different clusters.\n",
    "\n",
    "With other data sets, we have seen decent improvement with as little as ~100 training examples. Of course, performance will be better with  more examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "arB38jFwpgkK"
   },
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.3.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anyio<4,>=3.5.0 (from openai)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from openai) (1.8.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from openai) (1.10.13)\n",
      "Requirement already satisfied: tqdm>4 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
      "Requirement already satisfied: certifi in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/htahir1/.virtualenvs/temp/lib/python3.8/site-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Installing collected packages: httpcore, anyio, httpx, openai\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.0.0\n",
      "    Uninstalling anyio-4.0.0:\n",
      "      Successfully uninstalled anyio-4.0.0\n",
      "Successfully installed anyio-3.7.1 httpcore-1.0.2 httpx-0.25.1 openai-1.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ifvM7g4apgkK"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.embeddings_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split  \u001b[38;5;66;03m# for splitting train & test data\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m  \u001b[38;5;66;03m# for matrix optimization\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding, cosine_similarity  \u001b[38;5;66;03m# for embeddings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils.embeddings_utils'"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from typing import List, Tuple  # for type hints\n",
    "\n",
    "import numpy as np  # for manipulating arrays\n",
    "import pandas as pd  # for manipulating data in dataframes\n",
    "import pickle  # for saving the embeddings cache\n",
    "import plotly.express as px  # for plots\n",
    "import random  # for generating run IDs\n",
    "from sklearn.model_selection import train_test_split  # for splitting train & test data\n",
    "import torch  # for matrix optimization\n",
    "\n",
    "from utils.embeddings_utils import get_embedding, cosine_similarity  # for embeddings\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "DtBbryAapgkL"
   },
   "source": [
    "## 1. Inputs\n",
    "\n",
    "Most inputs are here. The key things to change are where to load your datset from, where to save a cache of embeddings to, and which embedding engine you want to use.\n",
    "\n",
    "Depending on how your data is formatted, you'll want to rewrite the process_input_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzxcWRCkpgkM"
   },
   "outputs": [],
   "source": [
    "# input parameters\n",
    "embedding_cache_path = \"data/snli_embedding_cache.pkl\"  # embeddings will be saved/loaded here\n",
    "default_embedding_engine = \"babbage-similarity\"  # text-embedding-ada-002 is recommended\n",
    "num_pairs_to_embed = 1000  # 1000 is arbitrary\n",
    "local_dataset_path = \"data/snli_1.0_train_2k.csv\"  # download from: https://nlp.stanford.edu/projects/snli/\n",
    "\n",
    "\n",
    "def process_input_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # you can customize this to preprocess your own dataset\n",
    "    # output should be a dataframe with 3 columns: text_1, text_2, label (1 for similar, -1 for dissimilar)\n",
    "    df[\"label\"] = df[\"gold_label\"]\n",
    "    df = df[df[\"label\"].isin([\"entailment\"])]\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: {\"entailment\": 1, \"contradiction\": -1}[x])\n",
    "    df = df.rename(columns={\"sentence1\": \"text_1\", \"sentence2\": \"text_2\"})\n",
    "    df = df[[\"text_1\", \"text_2\", \"label\"]]\n",
    "    df = df.head(num_pairs_to_embed)\n",
    "    return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aBbH71hEpgkM"
   },
   "source": [
    "## 2. Load and process input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAKLjYG6pgkN",
    "outputId": "dc178688-e97d-4ad0-b26c-dff67b858966"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(local_dataset_path)\n",
    "\n",
    "# process input data\n",
    "df = process_input_data(df)  # this demonstrates training data containing only positives\n",
    "\n",
    "# view data\n",
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "z2F1cCoYpgkO"
   },
   "source": [
    "## 3. Split data into training test sets\n",
    "\n",
    "Note that it's important to split data into training and test sets *before* generating synethetic negatives or positives. You don't want any text strings in the training data to show up in the test data. If there's contamination, the test metrics will look better than they'll actually be in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50QmnH2qpgkO",
    "outputId": "6144029b-eb29-439e-9990-7aeb28168e56"
   },
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "test_fraction = 0.5  # 0.5 is fairly arbitrary\n",
    "random_seed = 123  # random seed is arbitrary, but is helpful in reproducibility\n",
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=test_fraction, stratify=df[\"label\"], random_state=random_seed\n",
    ")\n",
    "train_df.loc[:, \"dataset\"] = \"train\"\n",
    "test_df.loc[:, \"dataset\"] = \"test\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MzAFkA2opgkP"
   },
   "source": [
    "## 4. Generate synthetic negatives\n",
    "\n",
    "This is another piece of the code that you will need to modify to match your use case.\n",
    "\n",
    "If you have data with positives and negatives, you can skip this section.\n",
    "\n",
    "If you have data with only positives, you can mostly keep it as is, where it generates negatives only.\n",
    "\n",
    "If you have multiclass data, you will want to generate both positives and negatives. The positives can be pairs of text that share labels, and the negatives can be pairs of text that do not share labels.\n",
    "\n",
    "The final output should be a dataframe with text pairs, where each pair is labeled -1 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUYd9V0zpgkP"
   },
   "outputs": [],
   "source": [
    "# generate negatives\n",
    "def dataframe_of_negatives(dataframe_of_positives: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return dataframe of negative pairs made by combining elements of positive pairs.\"\"\"\n",
    "    texts = set(dataframe_of_positives[\"text_1\"].values) | set(\n",
    "        dataframe_of_positives[\"text_2\"].values\n",
    "    )\n",
    "    all_pairs = {(t1, t2) for t1 in texts for t2 in texts if t1 < t2}\n",
    "    positive_pairs = set(\n",
    "        tuple(text_pair)\n",
    "        for text_pair in dataframe_of_positives[[\"text_1\", \"text_2\"]].values\n",
    "    )\n",
    "    negative_pairs = all_pairs - positive_pairs\n",
    "    df_of_negatives = pd.DataFrame(list(negative_pairs), columns=[\"text_1\", \"text_2\"])\n",
    "    df_of_negatives[\"label\"] = -1\n",
    "    return df_of_negatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rkh8-J89pgkP"
   },
   "outputs": [],
   "source": [
    "negatives_per_positive = (\n",
    "    1  # it will work at higher values too, but more data will be slower\n",
    ")\n",
    "# generate negatives for training dataset\n",
    "train_df_negatives = dataframe_of_negatives(train_df)\n",
    "train_df_negatives[\"dataset\"] = \"train\"\n",
    "# generate negatives for test dataset\n",
    "test_df_negatives = dataframe_of_negatives(test_df)\n",
    "test_df_negatives[\"dataset\"] = \"test\"\n",
    "# sample negatives and combine with positives\n",
    "train_df = pd.concat(\n",
    "    [\n",
    "        train_df,\n",
    "        train_df_negatives.sample(\n",
    "            n=len(train_df) * negatives_per_positive, random_state=random_seed\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "test_df = pd.concat(\n",
    "    [\n",
    "        test_df,\n",
    "        test_df_negatives.sample(\n",
    "            n=len(test_df) * negatives_per_positive, random_state=random_seed\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = pd.concat([train_df, test_df])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8MVSLMSrpgkQ"
   },
   "source": [
    "## 5. Calculate embeddings and cosine similarities\n",
    "\n",
    "Here, I create a cache to save the embeddings. This is handy so that you don't have to pay again if you want to run the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6tWgS_ApgkQ"
   },
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, engine) -> embedding\n",
    "try:\n",
    "    with open(embedding_cache_path, \"rb\") as f:\n",
    "        embedding_cache = pickle.load(f)\n",
    "except FileNotFoundError:\n",
    "    precomputed_embedding_cache_path = \"https://cdn.openai.com/API/examples/data/snli_embedding_cache.pkl\"\n",
    "    embedding_cache = pd.read_pickle(precomputed_embedding_cache_path)\n",
    "\n",
    "\n",
    "# this function will get embeddings from the cache and save them there afterward\n",
    "def get_embedding_with_cache(\n",
    "    text: str,\n",
    "    engine: str = default_embedding_engine,\n",
    "    embedding_cache: dict = embedding_cache,\n",
    "    embedding_cache_path: str = embedding_cache_path,\n",
    ") -> list:\n",
    "    if (text, engine) not in embedding_cache.keys():\n",
    "        # if not in cache, call API to get embedding\n",
    "        embedding_cache[(text, engine)] = get_embedding(text, engine)\n",
    "        # save embeddings cache to disk after each update\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(text, engine)]\n",
    "\n",
    "\n",
    "# create column of embeddings\n",
    "for column in [\"text_1\", \"text_2\"]:\n",
    "    df[f\"{column}_embedding\"] = df[column].apply(get_embedding_with_cache)\n",
    "\n",
    "# create column of cosine similarity between embeddings\n",
    "df[\"cosine_similarity\"] = df.apply(\n",
    "    lambda row: cosine_similarity(row[\"text_1_embedding\"], row[\"text_2_embedding\"]),\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4pwn608LpgkQ"
   },
   "source": [
    "## 6. Plot distribution of cosine similarity\n",
    "\n",
    "Here we measure similarity of text using cosine similarity. In our experience, most distance functions (L1, L2, cosine similarity) all work about the same. Note that our embeddings are already normalized to length 1, so cosine similarity is equivalent to dot product.\n",
    "\n",
    "The graphs show how much the overlap there is between the distribution of cosine similarities for similar and dissimilar pairs. If there is a high amount of overlap, that means there are some dissimilar pairs with greater cosine similarity than some similar pairs.\n",
    "\n",
    "The accuracy I compute is the accuracy of a simple rule that predicts 'similar (1)' if the cosine similarity is above some threshold X and otherwise predicts 'dissimilar (0)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SoeDF8vqpgkQ",
    "outputId": "17db817e-1702-4089-c4e8-8ca32d294930"
   },
   "outputs": [],
   "source": [
    "# calculate accuracy (and its standard error) of predicting label=1 if similarity>x\n",
    "# x is optimized by sweeping from -1 to 1 in steps of 0.01\n",
    "def accuracy_and_se(cosine_similarity: float, labeled_similarity: int) -> Tuple[float]:\n",
    "    accuracies = []\n",
    "    for threshold_thousandths in range(-1000, 1000, 1):\n",
    "        threshold = threshold_thousandths / 1000\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for cs, ls in zip(cosine_similarity, labeled_similarity):\n",
    "            total += 1\n",
    "            if cs > threshold:\n",
    "                prediction = 1\n",
    "            else:\n",
    "                prediction = -1\n",
    "            if prediction == ls:\n",
    "                correct += 1\n",
    "        accuracy = correct / total\n",
    "        accuracies.append(accuracy)\n",
    "    a = max(accuracies)\n",
    "    n = len(cosine_similarity)\n",
    "    standard_error = (a * (1 - a) / n) ** 0.5  # standard error of binomial\n",
    "    return a, standard_error\n",
    "\n",
    "\n",
    "# check that training and test sets are balanced\n",
    "px.histogram(\n",
    "    df,\n",
    "    x=\"cosine_similarity\",\n",
    "    color=\"label\",\n",
    "    barmode=\"overlay\",\n",
    "    width=500,\n",
    "    facet_row=\"dataset\",\n",
    ").show()\n",
    "\n",
    "for dataset in [\"train\", \"test\"]:\n",
    "    data = df[df[\"dataset\"] == dataset]\n",
    "    a, se = accuracy_and_se(data[\"cosine_similarity\"], data[\"label\"])\n",
    "    print(f\"{dataset} accuracy: {a:0.1%} ± {1.96 * se:0.1%}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zHLxlnsApgkR"
   },
   "source": [
    "## 7. Optimize the matrix using the training data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z52V0x8IpgkR"
   },
   "outputs": [],
   "source": [
    "def embedding_multiplied_by_matrix(\n",
    "    embedding: List[float], matrix: torch.tensor\n",
    ") -> np.array:\n",
    "    embedding_tensor = torch.tensor(embedding).float()\n",
    "    modified_embedding = embedding_tensor @ matrix\n",
    "    modified_embedding = modified_embedding.detach().numpy()\n",
    "    return modified_embedding\n",
    "\n",
    "\n",
    "# compute custom embeddings and new cosine similarities\n",
    "def apply_matrix_to_embeddings_dataframe(matrix: torch.tensor, df: pd.DataFrame):\n",
    "    for column in [\"text_1_embedding\", \"text_2_embedding\"]:\n",
    "        df[f\"{column}_custom\"] = df[column].apply(\n",
    "            lambda x: embedding_multiplied_by_matrix(x, matrix)\n",
    "        )\n",
    "    df[\"cosine_similarity_custom\"] = df.apply(\n",
    "        lambda row: cosine_similarity(\n",
    "            row[\"text_1_embedding_custom\"], row[\"text_2_embedding_custom\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2ZSXu6spgkR"
   },
   "outputs": [],
   "source": [
    "def optimize_matrix(\n",
    "    modified_embedding_length: int = 2048,  # in my brief experimentation, bigger was better (2048 is length of babbage encoding)\n",
    "    batch_size: int = 100,\n",
    "    max_epochs: int = 100,\n",
    "    learning_rate: float = 100.0,  # seemed to work best when similar to batch size - feel free to try a range of values\n",
    "    dropout_fraction: float = 0.0,  # in my testing, dropout helped by a couple percentage points (definitely not necessary)\n",
    "    df: pd.DataFrame = df,\n",
    "    print_progress: bool = True,\n",
    "    save_results: bool = True,\n",
    ") -> torch.tensor:\n",
    "    \"\"\"Return matrix optimized to minimize loss on training data.\"\"\"\n",
    "    run_id = random.randint(0, 2 ** 31 - 1)  # (range is arbitrary)\n",
    "    # convert from dataframe to torch tensors\n",
    "    # e is for embedding, s for similarity label\n",
    "    def tensors_from_dataframe(\n",
    "        df: pd.DataFrame,\n",
    "        embedding_column_1: str,\n",
    "        embedding_column_2: str,\n",
    "        similarity_label_column: str,\n",
    "    ) -> Tuple[torch.tensor]:\n",
    "        e1 = np.stack(np.array(df[embedding_column_1].values))\n",
    "        e2 = np.stack(np.array(df[embedding_column_2].values))\n",
    "        s = np.stack(np.array(df[similarity_label_column].astype(\"float\").values))\n",
    "\n",
    "        e1 = torch.from_numpy(e1).float()\n",
    "        e2 = torch.from_numpy(e2).float()\n",
    "        s = torch.from_numpy(s).float()\n",
    "\n",
    "        return e1, e2, s\n",
    "\n",
    "    e1_train, e2_train, s_train = tensors_from_dataframe(\n",
    "        df[df[\"dataset\"] == \"train\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n",
    "    )\n",
    "    e1_test, e2_test, s_test = tensors_from_dataframe(\n",
    "        df[df[\"dataset\"] == \"test\"], \"text_1_embedding\", \"text_2_embedding\", \"label\"\n",
    "    )\n",
    "\n",
    "    # create dataset and loader\n",
    "    dataset = torch.utils.data.TensorDataset(e1_train, e2_train, s_train)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    # define model (similarity of projected embeddings)\n",
    "    def model(embedding_1, embedding_2, matrix, dropout_fraction=dropout_fraction):\n",
    "        e1 = torch.nn.functional.dropout(embedding_1, p=dropout_fraction)\n",
    "        e2 = torch.nn.functional.dropout(embedding_2, p=dropout_fraction)\n",
    "        modified_embedding_1 = e1 @ matrix  # @ is matrix multiplication\n",
    "        modified_embedding_2 = e2 @ matrix\n",
    "        similarity = torch.nn.functional.cosine_similarity(\n",
    "            modified_embedding_1, modified_embedding_2\n",
    "        )\n",
    "        return similarity\n",
    "\n",
    "    # define loss function to minimize\n",
    "    def mse_loss(predictions, targets):\n",
    "        difference = predictions - targets\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "\n",
    "    # initialize projection matrix\n",
    "    embedding_length = len(df[\"text_1_embedding\"].values[0])\n",
    "    matrix = torch.randn(\n",
    "        embedding_length, modified_embedding_length, requires_grad=True\n",
    "    )\n",
    "\n",
    "    epochs, types, losses, accuracies, matrices = [], [], [], [], []\n",
    "    for epoch in range(1, 1 + max_epochs):\n",
    "        # iterate through training dataloader\n",
    "        for a, b, actual_similarity in train_loader:\n",
    "            # generate prediction\n",
    "            predicted_similarity = model(a, b, matrix)\n",
    "            # get loss and perform backpropagation\n",
    "            loss = mse_loss(predicted_similarity, actual_similarity)\n",
    "            loss.backward()\n",
    "            # update the weights\n",
    "            with torch.no_grad():\n",
    "                matrix -= matrix.grad * learning_rate\n",
    "                # set gradients to zero\n",
    "                matrix.grad.zero_()\n",
    "        # calculate test loss\n",
    "        test_predictions = model(e1_test, e2_test, matrix)\n",
    "        test_loss = mse_loss(test_predictions, s_test)\n",
    "\n",
    "        # compute custom embeddings and new cosine similarities\n",
    "        apply_matrix_to_embeddings_dataframe(matrix, df)\n",
    "\n",
    "        # calculate test accuracy\n",
    "        for dataset in [\"train\", \"test\"]:\n",
    "            data = df[df[\"dataset\"] == dataset]\n",
    "            a, se = accuracy_and_se(data[\"cosine_similarity_custom\"], data[\"label\"])\n",
    "\n",
    "            # record results of each epoch\n",
    "            epochs.append(epoch)\n",
    "            types.append(dataset)\n",
    "            losses.append(loss.item() if dataset == \"train\" else test_loss.item())\n",
    "            accuracies.append(a)\n",
    "            matrices.append(matrix.detach().numpy())\n",
    "\n",
    "            # optionally print accuracies\n",
    "            if print_progress is True:\n",
    "                print(\n",
    "                    f\"Epoch {epoch}/{max_epochs}: {dataset} accuracy: {a:0.1%} ± {1.96 * se:0.1%}\"\n",
    "                )\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\"epoch\": epochs, \"type\": types, \"loss\": losses, \"accuracy\": accuracies}\n",
    "    )\n",
    "    data[\"run_id\"] = run_id\n",
    "    data[\"modified_embedding_length\"] = modified_embedding_length\n",
    "    data[\"batch_size\"] = batch_size\n",
    "    data[\"max_epochs\"] = max_epochs\n",
    "    data[\"learning_rate\"] = learning_rate\n",
    "    data[\"dropout_fraction\"] = dropout_fraction\n",
    "    data[\n",
    "        \"matrix\"\n",
    "    ] = matrices  # saving every single matrix can get big; feel free to delete/change\n",
    "    if save_results is True:\n",
    "        data.to_csv(f\"{run_id}_optimization_results.csv\", index=False)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlcUW-zEpgkS",
    "outputId": "4bd4bdff-628a-406f-fffe-aedbfad66446"
   },
   "outputs": [],
   "source": [
    "# example hyperparameter search\n",
    "# I recommend starting with max_epochs=10 while initially exploring\n",
    "results = []\n",
    "max_epochs = 30\n",
    "dropout_fraction = 0.2\n",
    "for batch_size, learning_rate in [(10, 10), (100, 100), (1000, 1000)]:\n",
    "    result = optimize_matrix(\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        max_epochs=max_epochs,\n",
    "        dropout_fraction=dropout_fraction,\n",
    "        save_results=False,\n",
    "    )\n",
    "    results.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoTZWC1SpgkS",
    "outputId": "207360e5-fd07-4180-a143-0ec5dd27ffe1"
   },
   "outputs": [],
   "source": [
    "runs_df = pd.concat(results)\n",
    "\n",
    "# plot training loss and test loss over time\n",
    "px.line(\n",
    "    runs_df,\n",
    "    line_group=\"run_id\",\n",
    "    x=\"epoch\",\n",
    "    y=\"loss\",\n",
    "    color=\"type\",\n",
    "    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n",
    "    facet_row=\"learning_rate\",\n",
    "    facet_col=\"batch_size\",\n",
    "    width=500,\n",
    ").show()\n",
    "\n",
    "# plot accuracy over time\n",
    "px.line(\n",
    "    runs_df,\n",
    "    line_group=\"run_id\",\n",
    "    x=\"epoch\",\n",
    "    y=\"accuracy\",\n",
    "    color=\"type\",\n",
    "    hover_data=[\"batch_size\", \"learning_rate\", \"dropout_fraction\"],\n",
    "    facet_row=\"learning_rate\",\n",
    "    facet_col=\"batch_size\",\n",
    "    width=500,\n",
    ").show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MiBQDcMPpgkS"
   },
   "source": [
    "## 8. Plot the before & after, showing the results of the best matrix found during training\n",
    "\n",
    "The better the matrix is, the more cleanly it will separate the similar and dissimilar pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzjoyLDOpgkS"
   },
   "outputs": [],
   "source": [
    "# apply result of best run to original data\n",
    "best_run = runs_df.sort_values(by=\"accuracy\", ascending=False).iloc[0]\n",
    "best_matrix = best_run[\"matrix\"]\n",
    "apply_matrix_to_embeddings_dataframe(best_matrix, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLnvABnXpgkS",
    "outputId": "0c070faa-6e3e-4765-b082-565c72a609be"
   },
   "outputs": [],
   "source": [
    "# plot similarity distribution BEFORE customization\n",
    "px.histogram(\n",
    "    df,\n",
    "    x=\"cosine_similarity\",\n",
    "    color=\"label\",\n",
    "    barmode=\"overlay\",\n",
    "    width=500,\n",
    "    facet_row=\"dataset\",\n",
    ").show()\n",
    "\n",
    "test_df = df[df[\"dataset\"] == \"test\"]\n",
    "a, se = accuracy_and_se(test_df[\"cosine_similarity\"], test_df[\"label\"])\n",
    "print(f\"Test accuracy: {a:0.1%} ± {1.96 * se:0.1%}\")\n",
    "\n",
    "# plot similarity distribution AFTER customization\n",
    "px.histogram(\n",
    "    df,\n",
    "    x=\"cosine_similarity_custom\",\n",
    "    color=\"label\",\n",
    "    barmode=\"overlay\",\n",
    "    width=500,\n",
    "    facet_row=\"dataset\",\n",
    ").show()\n",
    "\n",
    "a, se = accuracy_and_se(test_df[\"cosine_similarity_custom\"], test_df[\"label\"])\n",
    "print(f\"Test accuracy after customization: {a:0.1%} ± {1.96 * se:0.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XO7iqiVjpgkT",
    "outputId": "a100a9e0-d5aa-46ab-b8a7-4ec6f7bd1cec"
   },
   "outputs": [],
   "source": [
    "best_matrix  # this is what you can multiply your embeddings by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "customized_embeddings_example_with_synthetic_negatives.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
