{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bdd511-4fc0-4bbd-9f4b-df76bbcb756a",
   "metadata": {},
   "source": [
    "# Finetune an embeddings model using ZenML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986c191-629b-45a2-8edb-472c96daddeb",
   "metadata": {},
   "source": [
    "In this notebook, we generate a synthetic dataset of (query, relevant documents) pairs from a corpus of documents *without labelers* by leveraging LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c262e939-9eef-421e-8a94-c1d8a6cf861d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.readers import SimpleWebPageReader\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import MetadataMode\n",
    "from zenml import step\n",
    "from typing import Any, Annotated, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf .zen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/wjayesh/apps/zenml\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: alembic<1.9.0,>=1.8.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (1.8.1)\n",
      "Requirement already satisfied: azure-mgmt-resource>=21.0.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (23.0.1)\n",
      "Requirement already satisfied: bcrypt==4.0.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (4.0.1)\n",
      "Requirement already satisfied: click<8.1.4,>=8.0.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (8.1.3)\n",
      "Requirement already satisfied: click-params<0.4.0,>=0.3.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.3.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (2.2.1)\n",
      "Requirement already satisfied: distro<2.0.0,>=1.6.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (1.8.0)\n",
      "Requirement already satisfied: docker<6.2.0,>=6.1.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (6.1.3)\n",
      "Requirement already satisfied: gitpython<4.0.0,>=3.1.18 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (3.1.40)\n",
      "Requirement already satisfied: httplib2<0.20,>=0.19.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.19.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (2.1.3)\n",
      "Requirement already satisfied: passlib<1.8.0,>=1.7.4 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from passlib[bcrypt]<1.8.0,>=1.7.4) (1.7.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (5.9.6)\n",
      "Requirement already satisfied: pydantic<1.11,>=1.9.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (1.10.13)\n",
      "Requirement already satisfied: pymysql<1.1.0,>=1.0.2 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (1.0.3)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (2.4.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (2.8.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (6.0.1)\n",
      "Requirement already satisfied: rich>=12.0.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from rich[jupyter]>=12.0.0) (13.7.0)\n",
      "Requirement already satisfied: sqlalchemy_utils==0.38.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.38.3)\n",
      "Requirement already satisfied: sqlmodel==0.0.8 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.0.8)\n",
      "Requirement already satisfied: Jinja2 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (3.1.2)\n",
      "Requirement already satisfied: fastapi<0.100,>=0.75 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.89.1)\n",
      "Requirement already satisfied: fastapi-utils<0.3.0,>=0.2.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.2.1)\n",
      "Requirement already satisfied: ipinfo>=4.4.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (4.4.3)\n",
      "Requirement already satisfied: orjson<3.9.0,>=3.8.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (3.8.14)\n",
      "Requirement already satisfied: pyjwt==2.7.* in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from pyjwt[crypto]==2.7.*; extra == \"server\") (2.7.0)\n",
      "Requirement already satisfied: python-multipart<0.1.0,>=0.0.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (0.0.6)\n",
      "Requirement already satisfied: uvicorn>=0.17.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\") (0.24.0.post1)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from pyjwt[crypto]==2.7.*; extra == \"server\") (41.0.5)\n",
      "Requirement already satisfied: SQLAlchemy>=1.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from sqlalchemy_utils==0.38.3) (1.4.41)\n",
      "Requirement already satisfied: sqlalchemy2-stubs in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from sqlmodel==0.0.8) (0.0.2a37)\n",
      "Requirement already satisfied: Mako in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from alembic<1.9.0,>=1.8.1) (1.3.0)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from azure-mgmt-resource>=21.0.0) (0.6.1)\n",
      "Requirement already satisfied: azure-common~=1.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from azure-mgmt-resource>=21.0.0) (1.1.28)\n",
      "Requirement already satisfied: azure-mgmt-core<2.0.0,>=1.3.2 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from azure-mgmt-resource>=21.0.0) (1.4.0)\n",
      "Requirement already satisfied: validators<0.19,>=0.18 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from click-params<0.4.0,>=0.3.0) (0.18.2)\n",
      "Requirement already satisfied: packaging>=14.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from docker<6.2.0,>=6.1.0) (23.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from docker<6.2.0,>=6.1.0) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from docker<6.2.0,>=6.1.0) (1.26.18)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from docker<6.2.0,>=6.1.0) (1.6.4)\n",
      "Requirement already satisfied: starlette==0.22.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from fastapi<0.100,>=0.75) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from starlette==0.22.0->fastapi<0.100,>=0.75) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from starlette==0.22.0->fastapi<0.100,>=0.75) (4.8.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from gitpython<4.0.0,>=3.1.18) (4.0.11)\n",
      "Requirement already satisfied: cachetools in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipinfo>=4.4.3) (5.3.2)\n",
      "Requirement already satisfied: aiohttp<=4 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipinfo>=4.4.3) (3.9.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from pandas>=1.1.5) (1.26.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from pandas>=1.1.5) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from pandas>=1.1.5) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.8.1) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from rich>=12.0.0->rich[jupyter]>=12.0.0) (2.17.2)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.5.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from rich[jupyter]>=12.0.0) (8.1.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn>=0.17.5->uvicorn[standard]>=0.17.5; extra == \"server\") (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\") (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\") (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\") (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\") (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from uvicorn[standard]>=0.17.5; extra == \"server\") (12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from Jinja2) (2.1.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from aiohttp<=4->ipinfo>=4.4.3) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from aiohttp<=4->ipinfo>=4.4.3) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from aiohttp<=4->ipinfo>=4.4.3) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from aiohttp<=4->ipinfo>=4.4.3) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from aiohttp<=4->ipinfo>=4.4.3) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from aiohttp<=4->ipinfo>=4.4.3) (4.0.3)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.26.2 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from azure-mgmt-core<2.0.0,>=1.3.2->azure-mgmt-resource>=21.0.0) (1.29.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from cryptography>=3.4.0->pyjwt[crypto]==2.7.*; extra == \"server\") (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.18) (5.0.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.2.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (8.17.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (5.13.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (3.0.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->rich[jupyter]>=12.0.0) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from requests>=2.26.0->docker<6.2.0,>=6.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from requests>=2.26.0->docker<6.2.0,>=6.1.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from requests>=2.26.0->docker<6.2.0,>=6.1.0) (2023.11.17)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from SQLAlchemy>=1.3->sqlalchemy_utils==0.38.3) (3.0.1)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from validators<0.19,>=0.18->click-params<0.4.0,>=0.3.0) (5.1.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<0.100,>=0.75) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<0.100,>=0.75) (1.2.0)\n",
      "Requirement already satisfied: pycparser in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]==2.7.*; extra == \"server\") (2.21)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (3.0.41)\n",
      "Requirement already satisfied: stack-data in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/wjayesh/llama-finetuning/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets<9,>=7.5.1->rich[jupyter]>=12.0.0) (0.2.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: zenml\n",
      "  Building editable for zenml (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zenml: filename=zenml-0.47.0-py3-none-any.whl size=11177 sha256=90e9992db942e0e1c596173204b5e8f568de1d8dba3dc1baa718e65c72167698\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4pjuf3a7/wheels/d1/f1/54/c0a0c45507a3f8878c132f0834a0fe67bf43a9f40063dae9d4\n",
      "Successfully built zenml\n",
      "Installing collected packages: zenml\n",
      "  Attempting uninstall: zenml\n",
      "    Found existing installation: zenml 0.47.0\n",
      "    Uninstalling zenml-0.47.0:\n",
      "      Successfully uninstalled zenml-0.47.0\n",
      "Successfully installed zenml-0.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ~/apps/zenml[server]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mNote: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "\u001b[1;35mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[1;35mSetting the repo active workspace to 'default'.\u001b[0m\n",
      "\u001b[33mSetting the repo active stack to default.\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mZenML repository initialized at \u001b[0m\u001b[2;35m/home/wjayesh/apps/\u001b[0m\u001b[2;95mzenml-rag-llm-finetune.\u001b[0m\n",
      "\u001b[2;32m⠹\u001b[0m\u001b[2;36m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mThe local active stack was initialized to \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m. This local configuration \u001b[0m\n",
      "\u001b[2;36mwill only take effect when you're running ZenML from the initialized repository \u001b[0m\n",
      "\u001b[2;36mroot, or from a subdirectory. For more information on repositories and \u001b[0m\n",
      "\u001b[2;36mconfigurations, please visit \u001b[0m\n",
      "\u001b[2;4;94mhttps://docs.zenml.io/user-guide/starter-guide/understand-stacks.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape all URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.finetune_pipeline.url_scraper.url_scraping_utils import get_all_pages, get_nested_readme_urls\n",
    "\n",
    "\n",
    "@step(enable_cache=True)\n",
    "def url_scraper(\n",
    "    docs_url: str = \"\",\n",
    "    repo_url: str = \"\",\n",
    "    release_notes_url: str = \"\",\n",
    "    website_url: str = \"\",\n",
    ") -> Tuple[Annotated[List, \"train_urls\"], Annotated[List, \"val_urls\"]]:\n",
    "    \"\"\"Generates a list of relevant URLs to scrape.\n",
    "\n",
    "    Args:\n",
    "        docs_url: URL to the documentation.\n",
    "        repo_url: URL to the repository.\n",
    "        release_notes_url: URL to the release notes.\n",
    "        website_url: URL to the website.\n",
    "\n",
    "    Returns:\n",
    "        List of URLs to scrape.\n",
    "    \"\"\"\n",
    "    # examples_readme_urls = get_nested_readme_urls(repo_url)\n",
    "    # docs_urls = get_all_pages(docs_url, finetuning=True)\n",
    "    # website_urls = get_all_pages(website_url, finetuning=True)\n",
    "    # all_urls = docs_urls + website_urls + [release_notes_url]\n",
    "\n",
    "    # # split into train and val sets\n",
    "    # train_urls = all_urls[: int(0.8 * len(all_urls))]\n",
    "    # val_urls = all_urls[int(0.8 * len(all_urls)) :]\n",
    "\n",
    "    return [website_url], [website_url]\n",
    "\n",
    "    return train_urls, val_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the contents of the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6ca90b0-eac9-420f-b6e9-a83749280b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step()\n",
    "def load_corpus(urls: List[str], verbose=False) -> Dict[str, str]:\n",
    "    if verbose:\n",
    "        print(f\"Loading URLs {urls}\")\n",
    "\n",
    "    reader = SimpleWebPageReader(html_to_text=True)\n",
    "    docs = reader.load_data(urls)\n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SimpleNodeParser.from_defaults()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "    corpus = {\n",
    "        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "        for node in nodes\n",
    "    }\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c26a5cd-9ec4-4c7b-bc58-9349d83a248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "@step()\n",
    "def generate_queries(\n",
    "    corpus: Dict[str, str],\n",
    "    num_questions_per_chunk: int = 2,\n",
    "    prompt_template: str = \"\",\n",
    "    verbose=False,\n",
    ") -> Tuple[Dict[str, str], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Automatically generate hypothetical questions that could be answered with\n",
    "    doc in the corpus.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=\"API_KEY\")\n",
    "\n",
    "    prompt_template = (\n",
    "        prompt_template\n",
    "        or \"\"\"\\\n",
    "    Context information is below.\n",
    "    \n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    \n",
    "    Given the context information and not prior knowledge.\n",
    "    generate only questions based on the below query.\n",
    "    \n",
    "    You are a Teacher/ Professor. Your task is to setup \\\n",
    "    {num_questions_per_chunk} questions for an upcoming \\\n",
    "    quiz/examination. The questions should be diverse in nature \\\n",
    "    across the document. Restrict the questions to the \\\n",
    "    context information provided.\"\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for node_id, text in tqdm(corpus.items()):\n",
    "        query = prompt_template.format(\n",
    "            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "        )\n",
    "        response = llm.complete(query)\n",
    "\n",
    "        result = str(response).strip().split(\"\\n\")\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "        ]\n",
    "        questions = [question for question in questions if len(question) > 0]\n",
    "\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [node_id]\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step()\n",
    "def merge_data(\n",
    "    train_corpus: Dict[str, str],\n",
    "    train_queries: Dict[str, str],\n",
    "    train_relevant_docs: Dict[str, List[str]],\n",
    "    val_corpus: Dict[str, str],\n",
    "    val_queries: Dict[str, str],\n",
    "    val_relevant_docs: Dict[str, List[str]],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    train_dataset = {\n",
    "        \"queries\": train_queries,\n",
    "        \"corpus\": train_corpus,\n",
    "        \"relevant_docs\": train_relevant_docs,\n",
    "    }\n",
    "\n",
    "    val_dataset = {\n",
    "        \"queries\": val_queries,\n",
    "        \"corpus\": val_corpus,\n",
    "        \"relevant_docs\": val_relevant_docs,\n",
    "    }\n",
    "\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "@step()\n",
    "def generate_training_examples(\n",
    "    dataset: Dict[str, Any], batch_size: int = 10\n",
    ") -> DataLoader:\n",
    "    \"\"\"Generate training examples from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing the corpus, queries and relevant docs.\n",
    "        batch_size: Batch size for the dataloader.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader containing the training examples.\n",
    "    \"\"\"\n",
    "    corpus = dataset['corpus']\n",
    "    queries = dataset['queries']\n",
    "    relevant_docs = dataset['relevant_docs']\n",
    "\n",
    "    examples = []\n",
    "    for query_id, query in queries.items():\n",
    "        node_id = relevant_docs[query_id][0]\n",
    "        text = corpus[node_id]\n",
    "        example = InputExample(texts=[query, text])\n",
    "        examples.append(example)\n",
    "\n",
    "    return DataLoader(examples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "\n",
    "@step()\n",
    "def create_evaluator(dataset: Dict[str, Any]) -> InformationRetrievalEvaluator:\n",
    "    \"\"\"Generate training examples from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset containing the corpus, queries and relevant docs.\n",
    "\n",
    "    Returns:\n",
    "        InformationRetrievalEvaluator for the dataset.\n",
    "    \"\"\"\n",
    "    corpus = dataset[\"corpus\"]\n",
    "    queries = dataset[\"queries\"]\n",
    "    relevant_docs = dataset[\"relevant_docs\"]\n",
    "\n",
    "    return InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tune an embeddings model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from zenml.artifacts.artifact_config import ArtifactConfig\n",
    "\n",
    "\n",
    "@step()\n",
    "def finetune_sentencetransformer_model(\n",
    "    loader: DataLoader,\n",
    "    evaluator: InformationRetrievalEvaluator,\n",
    "    EPOCHS: int = 2,\n",
    "    model_id: Optional[str] = \"BAAI/bge-small-en\",\n",
    ") -> Annotated[SentenceTransformer, ArtifactConfig(name=\"finetuned-sentence-transformer\", is_model_artifact=True)]:\n",
    "    model = SentenceTransformer(model_id)\n",
    "    loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "    warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        show_progress_bar=True,\n",
    "        evaluator=evaluator, \n",
    "        evaluation_steps=50,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline\n",
    "from zenml.model.model_version import ModelVersion\n",
    "\n",
    "# from steps.model_log_register import register_model\n",
    "\n",
    "\n",
    "@pipeline(\n",
    "    name=\"finetuning_pipeline\",\n",
    "    enable_cache=True,\n",
    "    model_version=ModelVersion(\n",
    "        name=\"finetuned-sentence-transformer\",\n",
    "        license=\"Apache\",\n",
    "        description=\"Custom Embeddings model\",\n",
    "        create_new_model_version=True,\n",
    "        delete_new_version_on_failure=True,\n",
    "    ),\n",
    ")\n",
    "def finetuning_pipeline(\n",
    "    docs_url: str = \"\",\n",
    "    repo_url: str = \"\",\n",
    "    release_notes_url: str = \"\",\n",
    "    website_url: str = \"\",\n",
    "):\n",
    "    train_urls, val_urls = url_scraper(\n",
    "        docs_url, repo_url, release_notes_url, website_url\n",
    "    )\n",
    "    train_corpus = load_corpus(train_urls, id=\"train_loader\")\n",
    "    val_corpus = load_corpus(val_urls, id=\"val_loader\")\n",
    "    train_queries, train_relevant_docs = generate_queries(\n",
    "        train_corpus, id=\"train_queries_generator\"\n",
    "    )\n",
    "    val_queries, val_relevant_docs = generate_queries(\n",
    "        val_corpus, id=\"val_queries_generator\"\n",
    "    )\n",
    "    train_dataset, val_dataset = merge_data(\n",
    "        train_corpus,\n",
    "        train_queries,\n",
    "        train_relevant_docs,\n",
    "        val_corpus,\n",
    "        val_queries,\n",
    "        val_relevant_docs,\n",
    "    )\n",
    "    training_examples = generate_training_examples(train_dataset)\n",
    "    evaluator = create_evaluator(val_dataset)\n",
    "    model = finetune_sentencetransformer_model(loader=training_examples, evaluator=evaluator, model_id=\"paraphrase-albert-small-v2\")\n",
    "    # register_model(model, \"finetuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mfinetuning_pipeline\u001b[1;35m.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered new version: \u001b[0m\u001b[1;36m(version 6)\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mNew model version \u001b[0m\u001b[1;36m3\u001b[1;35m was created.\u001b[0m\n",
      "\u001b[1;35mExecuting a new run.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mCaching \u001b[0m\u001b[1;36menabled\u001b[1;35m explicitly for \u001b[0m\u001b[1;36murl_scraper\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36murl_scraper\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36murl_scraper\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mtrain_loader\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mval_loader\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mval_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mtrain_queries_generator\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput_0\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput_1\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_queries_generator\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mval_queries_generator\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput_0\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput_1\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mval_queries_generator\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mmerge_data\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput_1\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput_0\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmerge_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mcreate_evaluator\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mcreate_evaluator\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mgenerate_training_examples\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mLinking artifact \u001b[0m\u001b[1;36moutput\u001b[1;35m to model \u001b[0m\u001b[1;36mNone\u001b[1;35m version \u001b[0m\u001b[1;36mNone\u001b[1;35m implicitly.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mgenerate_training_examples\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mfinetune_sentencetransformer_model\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mLoad pretrained SentenceTransformer: paraphrase-albert-small-v2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f6b7650f07405b8afdf9f1cacdb1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d1fdc383c84f8299ea348735529d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5868c8206fe472cb2610fb9cafdf8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea3707154ef4f5e8d884d62ea8bb1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/827 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07571cab07b41fca11139a049b213e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8542489826c9470796f989058be2bc3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/46.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34df4470c1e640929480f88e303397aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3499f57384479fb7a3d2329d5a133a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2b1b4aad0444109046639c86be3198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1f689372a41fdbaa18d6440651298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da63985adec41f6b986c159a719f4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0fc75ed54c4f0ab1a4fb2242b7712c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUse pytorch device: cpu\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a4a38d4bfc456289a43a880a38f67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3c6b79bcb7475b868ee2c72ad17211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInformation Retrieval Evaluation on  dataset in epoch 0 after 1 steps:\u001b[0m\n",
      "\u001b[1;35mQueries: 14\u001b[0m\n",
      "\u001b[1;35mCorpus: 7\n",
      "\u001b[0m\n",
      "\u001b[1;35mScore-Function: cos_sim\u001b[0m\n",
      "\u001b[1;35mAccuracy@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mAccuracy@3: 64.29%\u001b[0m\n",
      "\u001b[1;35mAccuracy@5: 85.71%\u001b[0m\n",
      "\u001b[1;35mAccuracy@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mPrecision@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mPrecision@3: 21.43%\u001b[0m\n",
      "\u001b[1;35mPrecision@5: 17.14%\u001b[0m\n",
      "\u001b[1;35mPrecision@10: 10.00%\u001b[0m\n",
      "\u001b[1;35mRecall@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mRecall@3: 64.29%\u001b[0m\n",
      "\u001b[1;35mRecall@5: 85.71%\u001b[0m\n",
      "\u001b[1;35mRecall@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mMRR@10: 0.4804\u001b[0m\n",
      "\u001b[1;35mNDCG@10: 0.6075\u001b[0m\n",
      "\u001b[1;35mMAP@100: 0.4804\u001b[0m\n",
      "\u001b[1;35mScore-Function: dot_score\u001b[0m\n",
      "\u001b[1;35mAccuracy@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mAccuracy@3: 71.43%\u001b[0m\n",
      "\u001b[1;35mAccuracy@5: 85.71%\u001b[0m\n",
      "\u001b[1;35mAccuracy@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mPrecision@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mPrecision@3: 23.81%\u001b[0m\n",
      "\u001b[1;35mPrecision@5: 17.14%\u001b[0m\n",
      "\u001b[1;35mPrecision@10: 10.00%\u001b[0m\n",
      "\u001b[1;35mRecall@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mRecall@3: 71.43%\u001b[0m\n",
      "\u001b[1;35mRecall@5: 85.71%\u001b[0m\n",
      "\u001b[1;35mRecall@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mMRR@10: 0.5459\u001b[0m\n",
      "\u001b[1;35mNDCG@10: 0.6558\u001b[0m\n",
      "\u001b[1;35mMAP@100: 0.5459\u001b[0m\n",
      "\u001b[1;35mInformation Retrieval Evaluation on  dataset in epoch 0 after 2 steps:\u001b[0m\n",
      "\u001b[1;35mQueries: 14\u001b[0m\n",
      "\u001b[1;35mCorpus: 7\n",
      "\u001b[0m\n",
      "\u001b[1;35mScore-Function: cos_sim\u001b[0m\n",
      "\u001b[1;35mAccuracy@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mAccuracy@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mAccuracy@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mAccuracy@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mPrecision@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mPrecision@3: 26.19%\u001b[0m\n",
      "\u001b[1;35mPrecision@5: 18.57%\u001b[0m\n",
      "\u001b[1;35mPrecision@10: 10.00%\u001b[0m\n",
      "\u001b[1;35mRecall@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mRecall@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mRecall@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mRecall@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mMRR@10: 0.5119\u001b[0m\n",
      "\u001b[1;35mNDCG@10: 0.6337\u001b[0m\n",
      "\u001b[1;35mMAP@100: 0.5119\u001b[0m\n",
      "\u001b[1;35mScore-Function: dot_score\u001b[0m\n",
      "\u001b[1;35mAccuracy@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mAccuracy@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mAccuracy@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mAccuracy@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mPrecision@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mPrecision@3: 26.19%\u001b[0m\n",
      "\u001b[1;35mPrecision@5: 18.57%\u001b[0m\n",
      "\u001b[1;35mPrecision@10: 10.00%\u001b[0m\n",
      "\u001b[1;35mRecall@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mRecall@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mRecall@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mRecall@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mMRR@10: 0.5833\u001b[0m\n",
      "\u001b[1;35mNDCG@10: 0.6865\u001b[0m\n",
      "\u001b[1;35mMAP@100: 0.5833\u001b[0m\n",
      "\u001b[1;35mInformation Retrieval Evaluation on  dataset after epoch 0:\u001b[0m\n",
      "\u001b[1;35mQueries: 14\u001b[0m\n",
      "\u001b[1;35mCorpus: 7\n",
      "\u001b[0m\n",
      "\u001b[1;35mScore-Function: cos_sim\u001b[0m\n",
      "\u001b[1;35mAccuracy@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mAccuracy@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mAccuracy@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mAccuracy@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mPrecision@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mPrecision@3: 26.19%\u001b[0m\n",
      "\u001b[1;35mPrecision@5: 18.57%\u001b[0m\n",
      "\u001b[1;35mPrecision@10: 10.00%\u001b[0m\n",
      "\u001b[1;35mRecall@1: 21.43%\u001b[0m\n",
      "\u001b[1;35mRecall@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mRecall@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mRecall@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mMRR@10: 0.5119\u001b[0m\n",
      "\u001b[1;35mNDCG@10: 0.6337\u001b[0m\n",
      "\u001b[1;35mMAP@100: 0.5119\u001b[0m\n",
      "\u001b[1;35mScore-Function: dot_score\u001b[0m\n",
      "\u001b[1;35mAccuracy@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mAccuracy@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mAccuracy@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mAccuracy@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mPrecision@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mPrecision@3: 26.19%\u001b[0m\n",
      "\u001b[1;35mPrecision@5: 18.57%\u001b[0m\n",
      "\u001b[1;35mPrecision@10: 10.00%\u001b[0m\n",
      "\u001b[1;35mRecall@1: 35.71%\u001b[0m\n",
      "\u001b[1;35mRecall@3: 78.57%\u001b[0m\n",
      "\u001b[1;35mRecall@5: 92.86%\u001b[0m\n",
      "\u001b[1;35mRecall@10: 100.00%\u001b[0m\n",
      "\u001b[1;35mMRR@10: 0.5833\u001b[0m\n",
      "\u001b[1;35mNDCG@10: 0.6865\u001b[0m\n",
      "\u001b[1;35mMAP@100: 0.5833\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mfinetune_sentencetransformer_model\u001b[1;35m has finished in \u001b[0m\u001b[1;36m24.474s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mRun \u001b[0m\u001b[1;36mfinetuning_pipeline-2023_11_28-08_51_50_717807\u001b[1;35m has finished in \u001b[0m\u001b[1;36m26.755s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml up\u001b[1;35m.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "version = \"0.47.0\"\n",
    "docs_url = f\"https://docs.zenml.io/v/{version}/\"\n",
    "website_url = \"https://zenml.io\"\n",
    "repo_url = f\"https://github.com/zenml-io/zenml/tree/{version}/examples\"\n",
    "release_notes_url = (\n",
    "    f\"https://github.com/zenml-io/zenml/blob/{version}/RELEASE_NOTES.md\"\n",
    ")\n",
    "\n",
    "finetuning_pipeline(\n",
    "    website_url=website_url,\n",
    "    docs_url=docs_url,\n",
    "    repo_url=repo_url,\n",
    "    release_notes_url=release_notes_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d8cc78b8-5c48-478e-919b-36ed845f82ff': 'What are the key features of ZenML that make it stand out from other ML orchestrators?', '1701e994-2c99-4c50-a4f3-76cd94d2db23': 'Which top companies have trusted ZenML for their MLOps workflows?', '2c09e019-573e-4d41-82ef-e980330cdfad': 'Which companies are featured in the context information? Provide the names of at least three companies.', '74bc5858-16e5-4f35-9c3f-84bc18a79010': 'What are the different types of images included in the context information? List at least two types.', 'eab371f3-9fde-4647-b24a-94d0aec28800': 'What is the main problem that companies face when it comes to creating their own ChatGPT?', '252d71a9-ee2b-462a-a70e-ee00530bd659': 'How does ZenML simplify the ML workflow for everyone on the team?', '0c4c704e-1a50-4a3d-88f0-3e43c2874d92': 'How does ZenML allow data scientists to focus on modeling and experimentation while ensuring their code is production-ready from the beginning?', 'c2e4df14-f945-4550-bfe2-a93c9c552535': 'What is the purpose of ZenML in the MLOps landscape and how does it differ from other individual tooling providers?', '5c4f5c61-3300-482b-89da-48b3bc7d2623': 'What are the target users of ZenML and how does it benefit them?', '0fac8f22-e566-4f3c-9378-5fe267c7fb59': 'What are the differences between the open-source and cloud versions of ZenML?', '19cf2eee-bbe4-4d73-b7dc-8bb0963cdf81': 'What are the benefits of using ZenML for ML teams? How does it help with tooling FOMO and vendor lock-in?', 'e465fe69-00bb-4dc3-af14-4521796603c9': 'How has ZenML helped WiseTech Global in setting up a pipeline for data and model versioning? What improvements do they look forward to in the future?', '305d8cbe-4ca3-41bb-a547-0890abff8abe': 'How does the website use cookies to enhance site navigation and assist in marketing efforts?', 'f5c59ee9-e4a6-46ca-bd2a-70bcbbc1f173': \"What are the different categories of storage mentioned in the Privacy Preference Center and how do they impact the user's experience on the website?\"}\n"
     ]
    }
   ],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "pipeline_model = Client().get_pipeline(\n",
    "    name_id_or_prefix=\"finetuning_pipeline\"\n",
    ")\n",
    "\n",
    "# you can additionally pass in the version if you want\n",
    "# to move between different pipeline implementations.\n",
    "# pipeline_model = Client().get_pipeline(\n",
    "#     name_id_or_prefix=PIPELINE_NAME, version=\"9\"\n",
    "# )\n",
    "\n",
    "if pipeline_model.runs is not None:\n",
    "    # get the last run\n",
    "    last_run = pipeline_model.runs[0]\n",
    "    # get the agent_creator step\n",
    "    queries_steps = last_run.steps[\"train_queries_generator\"]\n",
    "\n",
    "    try:\n",
    "        queries = queries_steps.outputs[\"output_0\"].load()\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
