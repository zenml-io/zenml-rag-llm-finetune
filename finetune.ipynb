{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1bdd511-4fc0-4bbd-9f4b-df76bbcb756a",
   "metadata": {},
   "source": [
    "# Finetune an embeddings model using ZenML Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986c191-629b-45a2-8edb-472c96daddeb",
   "metadata": {},
   "source": [
    "In this notebook, we generate a synthetic dataset of (query, relevant documents) pairs from a corpus of documents *without labelers* by leveraging LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c262e939-9eef-421e-8a94-c1d8a6cf861d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleWebPageReader\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.schema import MetadataMode\n",
    "from zenml import step\n",
    "from typing import Any, Annotated, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mNote: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "\u001b[1;35mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "\u001b[?25l\u001b[32m⠋\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠙\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠹\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠸\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\u001b[1;35mSetting the repo active workspace to 'default'.\u001b[0m\n",
      "\u001b[33mSetting the repo active stack to default.\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mZenML repository initialized at \u001b[0m\u001b[2;35m/home/wjayesh/apps/\u001b[0m\u001b[2;95mzenml-rag-llm-finetune.\u001b[0m\n",
      "\u001b[2;32m⠼\u001b[0m\u001b[2;36m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠼\u001b[0m Initializing ZenML repository at /home/wjayesh/apps/zenml-rag-llm-finetune.\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mThe local active stack was initialized to \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m. This local configuration \u001b[0m\n",
      "\u001b[2;36mwill only take effect when you're running ZenML from the initialized repository \u001b[0m\n",
      "\u001b[2;36mroot, or from a subdirectory. For more information on repositories and \u001b[0m\n",
      "\u001b[2;36mconfigurations, please visit \u001b[0m\n",
      "\u001b[2;4;94mhttps://docs.zenml.io/user-guide/starter-guide/understand-stacks.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape all URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.url_scraper.url_scraping_utils import get_all_pages, get_nested_readme_urls\n",
    "\n",
    "\n",
    "@step(enable_cache=True)\n",
    "def url_scraper(\n",
    "    docs_url: str = \"\",\n",
    "    repo_url: str = \"\",\n",
    "    release_notes_url: str = \"\",\n",
    "    website_url: str = \"\",\n",
    ") -> Tuple[Annotated[List, \"train_urls\"], Annotated[List, \"val_urls\"]]:\n",
    "    \"\"\"Generates a list of relevant URLs to scrape.\n",
    "\n",
    "    Args:\n",
    "        docs_url: URL to the documentation.\n",
    "        repo_url: URL to the repository.\n",
    "        release_notes_url: URL to the release notes.\n",
    "        website_url: URL to the website.\n",
    "\n",
    "    Returns:\n",
    "        List of URLs to scrape.\n",
    "    \"\"\"\n",
    "    # examples_readme_urls = get_nested_readme_urls(repo_url)\n",
    "    # docs_urls = get_all_pages(docs_url, finetuning=True)\n",
    "    # website_urls = get_all_pages(website_url, finetuning=True)\n",
    "    # all_urls = docs_urls + website_urls + [release_notes_url]\n",
    "\n",
    "    # # split into train and val sets\n",
    "    # train_urls = all_urls[: int(0.8 * len(all_urls))]\n",
    "    # val_urls = all_urls[int(0.8 * len(all_urls)) :]\n",
    "\n",
    "    return [website_url], [website_url]\n",
    "\n",
    "    return train_urls, val_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the contents of the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ca90b0-eac9-420f-b6e9-a83749280b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step()\n",
    "def load_corpus(urls: List[str], verbose=False) -> Dict[str, str]:\n",
    "    if verbose:\n",
    "        print(f\"Loading URLs {urls}\")\n",
    "\n",
    "    reader = SimpleWebPageReader(html_to_text=True)\n",
    "    docs = reader.load_data(urls)\n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "    parser = SimpleNodeParser.from_defaults()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Parsed {len(nodes)} nodes\")\n",
    "\n",
    "    corpus = {\n",
    "        node.node_id: node.get_content(metadata_mode=MetadataMode.NONE)\n",
    "        for node in nodes\n",
    "    }\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c26a5cd-9ec4-4c7b-bc58-9349d83a248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.schema import MetadataMode\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "@step()\n",
    "def generate_queries(\n",
    "    corpus: Dict[str, str],\n",
    "    num_questions_per_chunk: int = 2,\n",
    "    prompt_template: str = \"\",\n",
    "    verbose=False,\n",
    ") -> Tuple[Dict[str, str], Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Automatically generate hypothetical questions that could be answered with\n",
    "    doc in the corpus.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=\"API_KEY\")\n",
    "\n",
    "    prompt_template = (\n",
    "        prompt_template\n",
    "        or \"\"\"\\\n",
    "    Context information is below.\n",
    "    \n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    \n",
    "    Given the context information and not prior knowledge.\n",
    "    generate only questions based on the below query.\n",
    "    \n",
    "    You are a Teacher/ Professor. Your task is to setup \\\n",
    "    {num_questions_per_chunk} questions for an upcoming \\\n",
    "    quiz/examination. The questions should be diverse in nature \\\n",
    "    across the document. Restrict the questions to the \\\n",
    "    context information provided.\"\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for node_id, text in tqdm(corpus.items()):\n",
    "        query = prompt_template.format(\n",
    "            context_str=text, num_questions_per_chunk=num_questions_per_chunk\n",
    "        )\n",
    "        response = llm.complete(query)\n",
    "\n",
    "        result = str(response).strip().split(\"\\n\")\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "        ]\n",
    "        questions = [question for question in questions if len(question) > 0]\n",
    "\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [node_id]\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step()\n",
    "def merge_data(\n",
    "    train_corpus: Dict[str, str],\n",
    "    train_queries: Dict[str, str],\n",
    "    train_relevant_docs: Dict[str, List[str]],\n",
    "    val_corpus: Dict[str, str],\n",
    "    val_queries: Dict[str, str],\n",
    "    val_relevant_docs: Dict[str, List[str]],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    train_dataset = {\n",
    "        \"queries\": train_queries,\n",
    "        \"corpus\": train_corpus,\n",
    "        \"relevant_docs\": train_relevant_docs,\n",
    "    }\n",
    "\n",
    "    val_dataset = {\n",
    "        \"queries\": val_queries,\n",
    "        \"corpus\": val_corpus,\n",
    "        \"relevant_docs\": val_relevant_docs,\n",
    "    }\n",
    "\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "\n",
    "@step()\n",
    "def generate_training_examples(\n",
    "    dataset: Dict[str, Any], batch_size: int = 10\n",
    ") -> DataLoader:\n",
    "    \"\"\"Generate training examples from the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset containing the corpus, queries and relevant docs.\n",
    "        batch_size: Batch size for the dataloader.\n",
    "        \n",
    "    Returns:\n",
    "        DataLoader containing the training examples.\n",
    "    \"\"\"\n",
    "    corpus = dataset['corpus']\n",
    "    queries = dataset['queries']\n",
    "    relevant_docs = dataset['relevant_docs']\n",
    "\n",
    "    examples = []\n",
    "    for query_id, query in queries.items():\n",
    "        node_id = relevant_docs[query_id][0]\n",
    "        text = corpus[node_id]\n",
    "        example = InputExample(texts=[query, text])\n",
    "        examples.append(example)\n",
    "\n",
    "    return DataLoader(examples, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "\n",
    "\n",
    "@step()\n",
    "def create_evaluator(dataset: Dict[str, Any]) -> InformationRetrievalEvaluator:\n",
    "    \"\"\"Generate training examples from the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset containing the corpus, queries and relevant docs.\n",
    "\n",
    "    Returns:\n",
    "        InformationRetrievalEvaluator for the dataset.\n",
    "    \"\"\"\n",
    "    corpus = dataset[\"corpus\"]\n",
    "    queries = dataset[\"queries\"]\n",
    "    relevant_docs = dataset[\"relevant_docs\"]\n",
    "\n",
    "    return InformationRetrievalEvaluator(queries, corpus, relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tune an embeddings model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "\n",
    "\n",
    "@step()\n",
    "def finetune_sentencetransformer_model(\n",
    "    loader: DataLoader,\n",
    "    evaluator: InformationRetrievalEvaluator,\n",
    "    EPOCHS: int = 2,\n",
    "    model_id: Optional[str] = \"BAAI/bge-small-en\",\n",
    ") -> SentenceTransformer:\n",
    "    model = SentenceTransformer(model_id)\n",
    "    loss = losses.MultipleNegativesRankingLoss(model=model)\n",
    "\n",
    "    warmup_steps = int(len(loader) * EPOCHS * 0.1)\n",
    "\n",
    "    model.fit(\n",
    "        train_objectives=[(loader, loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=warmup_steps,\n",
    "        show_progress_bar=True,\n",
    "        evaluator=evaluator, \n",
    "        evaluation_steps=50,\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline\n",
    "\n",
    "@pipeline(name=\"finetuning_pipeline\", enable_cache=True)\n",
    "def finetuning_pipeline(\n",
    "    docs_url: str = \"\",\n",
    "    repo_url: str = \"\",\n",
    "    release_notes_url: str = \"\",\n",
    "    website_url: str = \"\",\n",
    "):\n",
    "    train_urls, val_urls = url_scraper(docs_url, repo_url, release_notes_url, website_url)\n",
    "    train_corpus = load_corpus(train_urls, id=\"train_loader\")\n",
    "    val_corpus = load_corpus(val_urls, id=\"val_loader\")\n",
    "    train_queries, train_relevant_docs = generate_queries(train_corpus, id=\"train_queries_generator\")\n",
    "    val_queries, val_relevant_docs = generate_queries(val_corpus, id=\"val_queries_generator\")\n",
    "    train_dataset, val_dataset = merge_data(\n",
    "        train_corpus,\n",
    "        train_queries,\n",
    "        train_relevant_docs,\n",
    "        val_corpus,\n",
    "        val_queries,\n",
    "        val_relevant_docs,\n",
    "    )\n",
    "    training_examples = generate_training_examples(train_dataset)\n",
    "    evaluator = create_evaluator(val_dataset)\n",
    "    model = finetune_sentencetransformer_model(training_examples, evaluator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mNote: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\u001b[0m\n",
      "\u001b[1;35mNumExpr defaulting to 8 threads.\u001b[0m\n",
      "\u001b[?25l\u001b[3m        Stack Configuration        \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mCOMPONENT_TYPE\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mCOMPONENT_NAME\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┠────────────────┼────────────────┨\n",
      "┃ ARTIFACT_STORE │ default        ┃\n",
      "┠────────────────┼────────────────┨\n",
      "┃ ORCHESTRATOR   │ default        ┃\n",
      "┗━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛\n",
      "\u001b[2;3m     'default' stack (ACTIVE)      \u001b[0m\n",
      "\u001b[32m⠋\u001b[0m Describing the stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mStack \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m with id \u001b[0m\u001b[2;32m'116dfeaa-40e5-47f2-b7c1-929aed5ee49f'\u001b[0m\u001b[2;36m is owned by user \u001b[0m\n",
      "\u001b[2;36mdefault and is \u001b[0m\u001b[2;32m'private'\u001b[0m\u001b[2;36m.\u001b[0m\n",
      "\u001b[2;32m⠋\u001b[0m\u001b[2;36m Describing the stack...\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32m⠋\u001b[0m Describing the stack...\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[33mYou can display various ZenML entities including pipelines, runs, stacks and \u001b[0m\n",
      "\u001b[33mmuch more on the ZenML Dashboard. You can try it locally, by running `zenml up`,\u001b[0m\n",
      "\u001b[33mor remotely, by deploying ZenML on the infrastructure of your choice.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mfinetuning_pipeline\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mRegistered new version: \u001b[0m\u001b[1;36m(version 5)\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mExecuting a new run.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mCaching \u001b[0m\u001b[1;36menabled\u001b[1;35m explicitly for \u001b[0m\u001b[1;36murl_scraper\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36murl_scraper\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36murl_scraper\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mtrain_loader\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[1;36mval_loader\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mval_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_queries_generator\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0afac0642b24b8fa9dee66d93082b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mtrain_queries_generator\u001b[1;35m has finished in \u001b[0m\u001b[1;36m10.090s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mval_queries_generator\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a6b4bc37ac494b929626e6bd46d203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mval_queries_generator\u001b[1;35m has finished in \u001b[0m\u001b[1;36m10.446s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmerge_data\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmerge_data\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.175s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mcreate_evaluator\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[33mNo materializer is registered for type \u001b[0m\u001b[1;36m<class 'sentence_transformers.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator'>\u001b[33m, so the default Pickle materializer was used. Pickle is not production ready and should only be used for prototyping as the artifacts cannot be loaded when running with a different Python version. Please consider implementing a custom materializer for type \u001b[0m\u001b[1;36m<class 'sentence_transformers.evaluation.InformationRetrievalEvaluator.InformationRetrievalEvaluator'>\u001b[33m according to the instructions at https://docs.zenml.io/user-guide/advanced-guide/artifact-management/handle-custom-data-types\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mcreate_evaluator\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.915s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mgenerate_training_examples\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mgenerate_training_examples\u001b[1;35m has finished in \u001b[0m\u001b[1;36m0.668s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mfinetune_sentencetransformer_model\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mLoad pretrained SentenceTransformer: BAAI/bge-small-en\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3ccd06eda249768c1b0d6cda955790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345abe4ab63b4c9bb6c0712c322a6d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b281afc36f6a480bb4bccfc19377fdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731373ab9c644bf2a979b2de0f1fa10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d96253f064404dbadce18484ffd55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0419c6e8ad1149c2be501308f62cb0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b211bcc4c324041966c962f78359adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309ba6bbe0004b55a300d0997ada59b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb5406c18124a97b94224576f478d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf13b6fde4e416083ddceca0d0da282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215c7d7c60364e94ae02c8c9da6589d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662f23e5faed493799a74791afc7030a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "591c5b493d5441b08c704c118055d379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUse pytorch device: cpu\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7654d27e36147ecbe3517e4ca45af24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83ab65d6023041bb8aeebe3b1d940f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "version = \"0.47.0\"\n",
    "docs_url = f\"https://docs.zenml.io/v/{version}/\"\n",
    "website_url = \"https://zenml.io\"\n",
    "repo_url = f\"https://github.com/zenml-io/zenml/tree/{version}/examples\"\n",
    "release_notes_url = (\n",
    "    f\"https://github.com/zenml-io/zenml/blob/{version}/RELEASE_NOTES.md\"\n",
    ")\n",
    "\n",
    "finetuning_pipeline(\n",
    "    website_url=website_url,\n",
    "    docs_url=docs_url,\n",
    "    repo_url=repo_url,\n",
    "    release_notes_url=release_notes_url,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a52951f0-9eae-4a53-b639-dd95f00c1e2e': 'What are some key features of ZenML that make it stand out from other ML orchestrators?', '6920463a-4630-4824-bcf1-f434567f9231': 'How does ZenML simplify the process of developing MLOps workflows?', 'c0a42c2b-d248-4cb2-8440-5450e99928b8': 'Which companies are featured in the context information provided?', '0675d6cd-eabc-4b6f-a574-a24363558b6a': 'What are the different types of logos or images included in the context information?', 'a9b76828-1b88-4d9d-abc6-f33e8ff4d548': 'What are some challenges that companies face when trying to implement machine learning internally?', '4a34ad70-df82-4791-912a-486e3ea41657': 'How does ZenML simplify the ML workflow for teams and bridge the gap between data science and operations?', '4db62f2e-e69d-4d27-8238-250fc885df77': 'What is the purpose of ZenML in the MLOps landscape? How does it differ from other tools?', 'e943b2d3-2c6c-4c57-8979-f516e8d750f7': 'How can ZenML be used to connect and organize data, models, and code across different tooling stacks?', 'e19e1caa-8c92-429f-8118-cbba8575af0a': 'How does ZenML help teams bridge the gap between data scientists and engineers in the ML stack?', '8ae6119b-e91b-465f-b6c6-72bc9ce89987': 'According to Richard Socher, what are the benefits of using ZenML for orchestrating ML pipelines?', 'ae1891b6-5761-43c8-8b0a-edd7ec989e0d': 'What are the key features of ZenML and how does it simplify MLOps?', '1872cb23-591c-4e53-8756-a3646eb3c6fe': 'How does ZenML compare to other orchestrators, experiment trackers, and end-to-end platforms in terms of functionality and capabilities?', 'ed56f42b-e5cf-4cc8-b59e-dac8fc1a267c': 'How do advertising networks use essential marketing items to deliver more relevant advertisements to users? How do these items also help measure the effectiveness of advertising campaigns?', '900ef7a4-c0aa-4b95-8618-d5d5649a9c24': 'In what ways do essential personalization items enhance the user experience on a website? Provide an example of how these items can provide personalized features based on user choices or location data.'}\n"
     ]
    }
   ],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "pipeline_model = Client().get_pipeline(\n",
    "    name_id_or_prefix=\"finetuning_pipeline\"\n",
    ")\n",
    "\n",
    "# you can additionally pass in the version if you want\n",
    "# to move between different pipeline implementations.\n",
    "# pipeline_model = Client().get_pipeline(\n",
    "#     name_id_or_prefix=PIPELINE_NAME, version=\"9\"\n",
    "# )\n",
    "\n",
    "if pipeline_model.runs is not None:\n",
    "    # get the last run\n",
    "    last_run = pipeline_model.runs[0]\n",
    "    # get the agent_creator step\n",
    "    queries_steps = last_run.steps[\"train_queries_generator\"]\n",
    "\n",
    "    try:\n",
    "        queries = queries_steps.outputs[\"output_0\"].load()\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
